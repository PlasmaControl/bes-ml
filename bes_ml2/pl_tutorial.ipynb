{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrsmith\u001b[0m (\u001b[33muw-bes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataclasses\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms\n",
    "\n",
    "import pyhessian\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(eq=False)\n",
    "class MNISTDataModule_DataClass():\n",
    "    data_dir: str = './MNIST'\n",
    "    batch_size: int = 32\n",
    "    fraction_validation: float = 0.2\n",
    "    seed: int = 42\n",
    "    num_workers: int = 8\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(eq=False)\n",
    "class MNISTDataModule(\n",
    "    pl.LightningDataModule,\n",
    "    MNISTDataModule_DataClass,\n",
    "):\n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        transform = torchvision.transforms.ToTensor()\n",
    "        mnist_full = torchvision.datasets.MNIST(\n",
    "            self.data_dir, \n",
    "            download=True,\n",
    "            train=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "        self.mnist_test = torchvision.datasets.MNIST(\n",
    "            self.data_dir, \n",
    "            download=True,\n",
    "            train=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        # partition training and validation data\n",
    "        valid_set_size = int(len(mnist_full) * self.fraction_validation)\n",
    "        train_set_size = len(mnist_full) - valid_set_size\n",
    "        seed = torch.Generator().manual_seed(self.seed)\n",
    "        self.mnist_train, self.mnist_val = torch.utils.data.random_split(\n",
    "            mnist_full, \n",
    "            [train_set_size, valid_set_size], \n",
    "            generator=seed,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.mnist_train, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.mnist_val, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.mnist_test, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(eq=False)\n",
    "class LitAutoEncoder_DataClass():\n",
    "    lr: float = 1e-3\n",
    "    hidden_size_1: int = 128\n",
    "    hidden_size_2: int = 32\n",
    "    hessian_epoch_interval: int = 2\n",
    "    dropout: float = 0.1\n",
    "    leaky_relu_slope: float = 1e-2\n",
    "\n",
    "@dataclasses.dataclass(eq=False)\n",
    "class LitAutoEncoder(\n",
    "    pl.LightningModule,\n",
    "    LitAutoEncoder_DataClass,\n",
    "):\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "        self.example_input_array = torch.Tensor(4,28,28)\n",
    "        self.save_hyperparameters()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(28 * 28, self.hidden_size_1),\n",
    "            torch.nn.LeakyReLU(negative_slope=self.leaky_relu_slope),\n",
    "            torch.nn.Dropout(p=self.dropout),\n",
    "            torch.nn.Linear(self.hidden_size_1, self.hidden_size_2),\n",
    "            torch.nn.LeakyReLU(negative_slope=self.leaky_relu_slope),\n",
    "            torch.nn.Dropout(p=self.dropout),\n",
    "            torch.nn.Linear(self.hidden_size_2, 10),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.loss_fn = F.cross_entropy\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def eval_and_loss(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        loss = self.eval_and_loss(batch)\n",
    "        self.log(\"tr_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.eval_and_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.eval_and_loss(batch)\n",
    "        self.log(\"hp_metric\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        if self.hessian_epoch_interval and self.current_epoch%self.hessian_epoch_interval == 0:\n",
    "            print('calc hess')\n",
    "            hessian = pyhessian.hessian(\n",
    "                model=self,\n",
    "                criterion=self.loss_fn,\n",
    "                dataloader=self.trainer.train_dataloader,\n",
    "                cuda=torch.cuda.is_available(),\n",
    "            )\n",
    "            print('calc eigenvals')\n",
    "            eigenvalues, eigenvectors = hessian.eigenvalues(top_n=1, maxIter=5, tol=1e-2)\n",
    "            print('finished hess')\n",
    "            for param in self.parameters():\n",
    "                param.grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name       | Type       | Params | In sizes    | Out sizes\n",
      "--------------------------------------------------------------------\n",
      "0 | classifier | Sequential | 104 K  | [4, 28, 28] | [4, 10]  \n",
      "--------------------------------------------------------------------\n",
      "104 K     Trainable params\n",
      "0         Non-trainable params\n",
      "104 K     Total params\n",
      "0.420     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a2fc16a6a046aeb1ee3219f9e476ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437888f825fc43d194c35594087a1bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b27afb531ad4880a8758683607136cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        hp_metric            2.30108904838562\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'hp_metric': 2.30108904838562}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "autoencoder = LitAutoEncoder(\n",
    "    hessian_epoch_interval=0,\n",
    ")\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "# check model and data\n",
    "check_trainer = pl.Trainer(\n",
    "    fast_dev_run=True,\n",
    ")\n",
    "check_trainer.fit(\n",
    "    model=autoencoder, \n",
    "    datamodule=datamodule,\n",
    ")\n",
    "check_trainer.test(\n",
    "    model=autoencoder,\n",
    "    datamodule=datamodule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Experiment_test/wandb/run-20230329_121657-kzb9i3g0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uw-bes/Experiment_test/runs/kzb9i3g0' target=\"_blank\">version_4</a></strong> to <a href='https://wandb.ai/uw-bes/Experiment_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uw-bes/Experiment_test' target=\"_blank\">https://wandb.ai/uw-bes/Experiment_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uw-bes/Experiment_test/runs/kzb9i3g0' target=\"_blank\">https://wandb.ai/uw-bes/Experiment_test/runs/kzb9i3g0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params | In sizes    | Out sizes\n",
      "--------------------------------------------------------------------\n",
      "0 | classifier | Sequential | 104 K  | [4, 28, 28] | [4, 10]  \n",
      "--------------------------------------------------------------------\n",
      "104 K     Trainable params\n",
      "0         Non-trainable params\n",
      "104 K     Total params\n",
      "0.420     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        hp_metric            1.541492223739624\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50ee1ea36304ff98a00b21ebf242374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.042 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.064139…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">version_4</strong> at: <a href='https://wandb.ai/uw-bes/Experiment_test/runs/kzb9i3g0' target=\"_blank\">https://wandb.ai/uw-bes/Experiment_test/runs/kzb9i3g0</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_dir = 'Experiment_test'\n",
    "\n",
    "autoencoder = LitAutoEncoder(\n",
    "    lr=1e-4,\n",
    "    hessian_epoch_interval=0,\n",
    ")\n",
    "\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir='.', \n",
    "    name=exp_dir, \n",
    "    log_graph=True,\n",
    ")\n",
    "tb_logger.log_graph(autoencoder)\n",
    "wandb_logger = WandbLogger(\n",
    "    save_dir=exp_dir,\n",
    "    project=exp_dir,\n",
    "    name=f\"version_{tb_logger.version}\"\n",
    ")\n",
    "wandb_logger.watch(autoencoder, log='all', log_freq=500)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=6,\n",
    "    logger=[\n",
    "        tb_logger,\n",
    "        wandb_logger,\n",
    "    ],\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=autoencoder, \n",
    "    datamodule=datamodule,\n",
    ")\n",
    "trainer.test(\n",
    "    model=autoencoder, \n",
    "    datamodule=datamodule,\n",
    ")\n",
    "\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = MNISTDataModule()\n",
    "\n",
    "# lr_values = [1e-3, 1e-4]\n",
    "hidden_size_1_values = [16, 32, 64]\n",
    "hidden_size_2_values = [2, 4, 8]\n",
    "\n",
    "exp_dir = 'Experiment_05'\n",
    "\n",
    "# for lr in lr_values:\n",
    "for hidden_size_1 in hidden_size_1_values:\n",
    "    for hidden_size_2 in hidden_size_2_values:\n",
    "\n",
    "        # define model\n",
    "        autoencoder = LitAutoEncoder(\n",
    "            # lr=lr,\n",
    "            hidden_size_1=hidden_size_1,\n",
    "            hidden_size_2=hidden_size_2,\n",
    "        )\n",
    "\n",
    "        tb_logger = TensorBoardLogger(\n",
    "            save_dir='.', \n",
    "            name=exp_dir, \n",
    "            log_graph=True,\n",
    "        )\n",
    "        tb_logger.log_graph(autoencoder)\n",
    "        wandb_logger = WandbLogger(\n",
    "            save_dir=exp_dir,\n",
    "            project=exp_dir,\n",
    "            name=f\"version_{tb_logger.version}\"\n",
    "        )\n",
    "        wandb_logger.watch(autoencoder, log='all', log_freq=500)\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=4,\n",
    "            logger=[\n",
    "                tb_logger,\n",
    "                wandb_logger,\n",
    "            ],\n",
    "            enable_progress_bar=False,\n",
    "        )\n",
    "        trainer.fit(\n",
    "            model=autoencoder, \n",
    "            datamodule=datamodule,\n",
    "        )\n",
    "        trainer.test(\n",
    "            model=autoencoder, \n",
    "            datamodule=datamodule,\n",
    "        )\n",
    "\n",
    "        wandb.finish(quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
